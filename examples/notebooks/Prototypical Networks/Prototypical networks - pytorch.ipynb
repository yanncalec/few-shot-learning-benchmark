{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe0c3e3",
   "metadata": {},
   "source": [
    "# Protypical Networks\n",
    "\n",
    "Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1c93bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "Logger = logging.getLogger('protonet')\n",
    "Logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# import skimage, skimage.io, skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d828a698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda is available.\n"
     ]
    }
   ],
   "source": [
    "import torch                                        # root package\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "# torch.manual_seed(time.time())\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n",
    "# import torch.autograd as autograd         # computation graph\n",
    "# from torch import Tensor                  # tensor node in the computation graph\n",
    "# import torch.nn as nn                     # neural networks\n",
    "# import torch.nn.functional as F           # layers, activations and more\n",
    "# from torch.jit import script, trace       # hybrid frontend decorator and tracing jit\n",
    "\n",
    "# import torchvision\n",
    "# from torchvision import datasets, models, transforms\n",
    "# print(torchvision.__version__)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device {} is available.'.format(device))\n",
    "\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a8abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4e430",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb9e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/home/han/Projects/Few Shot Learning/Notebooks/Prototype_Networks/outputs'\n"
     ]
    }
   ],
   "source": [
    "datadir = os.path.expanduser('~/Database/Machine_Learning')\n",
    "workdir = os.getcwd()\n",
    "\n",
    "outdir = f'{workdir}/outputs'\n",
    "\n",
    "try:\n",
    "    os.makedirs(outdir)\n",
    "except Exception as msg:\n",
    "    print(msg)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2850e9b",
   "metadata": {},
   "source": [
    "### Demonstration of meta-sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d729a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## On the original dataset\n",
    "# indir = f'{datadir}/Omniglot/omniglot.original/python/'\n",
    "\n",
    "# foo1 = nd.from_folder(f'{indir}/images_background')\n",
    "# foo2 = nd.from_folder(f'{indir}/images_evaluation')\n",
    "\n",
    "# data_dict = {'images_background': foo1, 'images_evaluation': foo2}\n",
    "# # print(len(data_dict.keys()))\n",
    "\n",
    "# ms = fsl.utils.data.Meta_Sampler(data_dict, 0.75, lvl_split=1, lvl_task=2, n_way=20, k_shot=5)\n",
    "\n",
    "# print(len(ms.trainval_dict.keys()))\n",
    "\n",
    "# print(len(ms.test_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9d077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## On the fused dataset\n",
    "indir = f'{datadir}/Omniglot/omniglot.fused/data/'\n",
    "\n",
    "data_dict = fsl.utils.nested_dict.from_folder(indir)\n",
    "train_dict, test_dict = fsl.utils.nested_dict.random_split_level(data_dict, 0.75, 0)\n",
    "\n",
    "train_sampler = fsl.utils.data.Meta_Sampler(train_dict, indir, lvl_task=1, n_way=10, k_shot=5, q_shot=None)\n",
    "test_sampler = fsl.utils.data.Meta_Sampler(test_dict, indir, lvl_task=1, n_way=10, k_shot=5, q_shot=None)\n",
    "\n",
    "# test_sampler = train_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c9023",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93879689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/han/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "kwargs_imread = dict(invert=True, dim=(20, 20))\n",
    "model = fsl.models.torch.ProtoNet((1, *kwargs_imread['dim']), use_cuda=use_cuda)\n",
    "\n",
    "# if use_cuda:\n",
    "#     model.cuda()\n",
    "# else:\n",
    "#     model.cpu()\n",
    "    \n",
    "decay_every = 20\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, decay_every, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274878f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25894d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-----Episode 1/100-----\n",
      "Task 1/10: loss.mean=1.258e-01, acc.mean=9.664e-01\n",
      "Task 2/10: loss.mean=5.531e-02, acc.mean=9.848e-01\n",
      "Task 3/10: loss.mean=4.302e-02, acc.mean=9.875e-01\n",
      "Task 4/10: loss.mean=3.999e-02, acc.mean=9.880e-01\n",
      "Task 5/10: loss.mean=4.950e-02, acc.mean=9.855e-01\n",
      "Task 6/10: loss.mean=3.761e-02, acc.mean=9.898e-01\n",
      "Task 7/10: loss.mean=4.229e-02, acc.mean=9.880e-01\n",
      "Task 8/10: loss.mean=2.961e-02, acc.mean=9.917e-01\n",
      "Task 9/10: loss.mean=2.281e-02, acc.mean=9.940e-01\n",
      "Task 10/10: loss.mean=2.600e-02, acc.mean=9.930e-01\n",
      "Validation: loss.mean=2.251e+00, acc.mean=3.687e-01\n",
      "\n",
      "-----Episode 2/100-----\n",
      "Task 1/10: loss.mean=2.135e-02, acc.mean=9.939e-01\n",
      "Task 2/10: loss.mean=3.342e-02, acc.mean=9.900e-01\n",
      "Task 3/10: loss.mean=1.886e-02, acc.mean=9.946e-01\n",
      "Task 4/10: loss.mean=1.830e-02, acc.mean=9.951e-01\n",
      "Task 5/10: loss.mean=2.898e-02, acc.mean=9.920e-01\n",
      "Task 6/10: loss.mean=2.174e-02, acc.mean=9.937e-01\n",
      "Task 7/10: loss.mean=2.642e-02, acc.mean=9.928e-01\n",
      "Task 8/10: loss.mean=1.489e-02, acc.mean=9.959e-01\n",
      "Task 9/10: loss.mean=2.135e-02, acc.mean=9.938e-01\n",
      "Task 10/10: loss.mean=1.784e-02, acc.mean=9.948e-01\n",
      "Validation: loss.mean=2.103e+00, acc.mean=4.933e-01\n",
      "\n",
      "-----Episode 3/100-----\n",
      "Task 1/10: loss.mean=2.351e-02, acc.mean=9.938e-01\n",
      "Task 2/10: loss.mean=2.654e-02, acc.mean=9.930e-01\n",
      "Task 3/10: loss.mean=1.929e-02, acc.mean=9.947e-01\n",
      "Task 4/10: loss.mean=1.842e-02, acc.mean=9.955e-01\n",
      "Task 5/10: loss.mean=2.585e-02, acc.mean=9.931e-01\n",
      "Task 6/10: loss.mean=2.016e-02, acc.mean=9.944e-01\n",
      "Task 7/10: loss.mean=1.616e-02, acc.mean=9.960e-01\n",
      "Task 8/10: loss.mean=2.662e-02, acc.mean=9.939e-01\n",
      "Task 9/10: loss.mean=1.911e-02, acc.mean=9.948e-01\n",
      "Task 10/10: loss.mean=2.155e-02, acc.mean=9.940e-01\n",
      "Validation: loss.mean=1.658e+00, acc.mean=5.867e-01\n",
      "\n",
      "-----Episode 4/100-----\n",
      "Task 1/10: loss.mean=2.964e-02, acc.mean=9.918e-01\n",
      "Task 2/10: loss.mean=8.888e-03, acc.mean=9.976e-01\n",
      "Task 3/10: loss.mean=2.363e-02, acc.mean=9.930e-01\n",
      "Task 4/10: loss.mean=2.249e-02, acc.mean=9.939e-01\n",
      "Task 5/10: loss.mean=1.157e-02, acc.mean=9.969e-01\n",
      "Task 6/10: loss.mean=1.338e-02, acc.mean=9.962e-01\n",
      "Task 7/10: loss.mean=2.468e-02, acc.mean=9.931e-01\n",
      "Task 8/10: loss.mean=1.517e-02, acc.mean=9.962e-01\n",
      "Task 9/10: loss.mean=2.644e-02, acc.mean=9.932e-01\n",
      "Task 10/10: loss.mean=1.842e-02, acc.mean=9.952e-01\n",
      "Validation: loss.mean=1.398e+00, acc.mean=6.180e-01\n",
      "\n",
      "-----Episode 5/100-----\n",
      "Task 1/10: loss.mean=2.423e-02, acc.mean=9.942e-01\n",
      "Task 2/10: loss.mean=1.447e-02, acc.mean=9.964e-01\n",
      "Task 3/10: loss.mean=2.819e-02, acc.mean=9.927e-01\n",
      "Task 4/10: loss.mean=2.805e-02, acc.mean=9.937e-01\n",
      "Task 5/10: loss.mean=3.548e-02, acc.mean=9.913e-01\n",
      "Task 6/10: loss.mean=2.348e-02, acc.mean=9.935e-01\n",
      "Task 7/10: loss.mean=2.571e-02, acc.mean=9.941e-01\n",
      "Task 8/10: loss.mean=1.680e-02, acc.mean=9.954e-01\n",
      "Task 9/10: loss.mean=1.942e-02, acc.mean=9.954e-01\n",
      "Task 10/10: loss.mean=2.060e-02, acc.mean=9.950e-01\n",
      "Validation: loss.mean=1.257e+00, acc.mean=6.333e-01\n",
      "\n",
      "-----Episode 6/100-----\n",
      "Task 1/10: loss.mean=2.763e-02, acc.mean=9.924e-01\n",
      "Task 2/10: loss.mean=2.499e-02, acc.mean=9.936e-01\n",
      "Task 3/10: loss.mean=1.004e-02, acc.mean=9.972e-01\n",
      "Task 4/10: loss.mean=1.807e-02, acc.mean=9.946e-01\n",
      "Task 5/10: loss.mean=2.010e-02, acc.mean=9.947e-01\n",
      "Task 6/10: loss.mean=1.971e-02, acc.mean=9.953e-01\n",
      "Task 7/10: loss.mean=2.123e-02, acc.mean=9.948e-01\n",
      "Task 8/10: loss.mean=1.439e-02, acc.mean=9.965e-01\n",
      "Task 9/10: loss.mean=1.727e-02, acc.mean=9.959e-01\n",
      "Task 10/10: loss.mean=2.294e-02, acc.mean=9.946e-01\n",
      "Validation: loss.mean=1.808e+00, acc.mean=5.507e-01\n",
      "\n",
      "-----Episode 7/100-----\n",
      "Task 1/10: loss.mean=3.801e-02, acc.mean=9.907e-01\n",
      "Task 2/10: loss.mean=2.458e-02, acc.mean=9.939e-01\n",
      "Task 3/10: loss.mean=3.031e-02, acc.mean=9.928e-01\n",
      "Task 4/10: loss.mean=3.317e-02, acc.mean=9.917e-01\n",
      "Task 5/10: loss.mean=2.999e-02, acc.mean=9.924e-01\n",
      "Task 6/10: loss.mean=3.729e-02, acc.mean=9.896e-01\n",
      "Task 7/10: loss.mean=3.205e-02, acc.mean=9.926e-01\n",
      "Task 8/10: loss.mean=1.640e-02, acc.mean=9.958e-01\n",
      "Task 9/10: loss.mean=3.254e-02, acc.mean=9.920e-01\n",
      "Task 10/10: loss.mean=5.002e-02, acc.mean=9.880e-01\n",
      "Validation: loss.mean=1.302e+00, acc.mean=6.353e-01\n",
      "\n",
      "-----Episode 8/100-----\n",
      "Task 1/10: loss.mean=2.496e-02, acc.mean=9.936e-01\n",
      "Task 2/10: loss.mean=4.086e-02, acc.mean=9.891e-01\n",
      "Task 3/10: loss.mean=3.149e-02, acc.mean=9.921e-01\n",
      "Task 4/10: loss.mean=3.418e-02, acc.mean=9.913e-01\n",
      "Task 5/10: loss.mean=3.397e-02, acc.mean=9.909e-01\n",
      "Task 6/10: loss.mean=1.644e-02, acc.mean=9.959e-01\n",
      "Task 7/10: loss.mean=2.130e-02, acc.mean=9.945e-01\n",
      "Task 8/10: loss.mean=2.505e-02, acc.mean=9.940e-01\n",
      "Task 9/10: loss.mean=3.319e-02, acc.mean=9.914e-01\n",
      "Task 10/10: loss.mean=1.705e-02, acc.mean=9.956e-01\n",
      "Validation: loss.mean=1.228e+00, acc.mean=6.687e-01\n",
      "\n",
      "-----Episode 9/100-----\n",
      "Task 1/10: loss.mean=2.464e-02, acc.mean=9.939e-01\n",
      "Task 2/10: loss.mean=4.515e-02, acc.mean=9.881e-01\n",
      "Task 3/10: loss.mean=5.396e-02, acc.mean=9.859e-01\n",
      "Task 4/10: loss.mean=6.518e-02, acc.mean=9.825e-01\n",
      "Task 5/10: loss.mean=8.167e-02, acc.mean=9.766e-01\n",
      "Task 6/10: loss.mean=6.666e-02, acc.mean=9.810e-01\n",
      "Task 7/10: loss.mean=6.912e-02, acc.mean=9.813e-01\n",
      "Task 8/10: loss.mean=6.502e-02, acc.mean=9.803e-01\n",
      "Task 9/10: loss.mean=6.037e-02, acc.mean=9.819e-01\n",
      "Task 10/10: loss.mean=8.235e-02, acc.mean=9.767e-01\n",
      "Validation: loss.mean=1.144e+00, acc.mean=7.160e-01\n",
      "\n",
      "-----Episode 10/100-----\n",
      "Task 1/10: loss.mean=4.047e-02, acc.mean=9.898e-01\n",
      "Task 2/10: loss.mean=1.019e-01, acc.mean=9.715e-01\n",
      "Task 3/10: loss.mean=4.134e-02, acc.mean=9.888e-01\n",
      "Task 4/10: loss.mean=1.588e-02, acc.mean=9.962e-01\n",
      "Task 5/10: loss.mean=9.039e-02, acc.mean=9.716e-01\n",
      "Task 6/10: loss.mean=1.248e-01, acc.mean=9.623e-01\n",
      "Task 7/10: loss.mean=2.430e-02, acc.mean=9.935e-01\n",
      "Task 8/10: loss.mean=2.512e-02, acc.mean=9.932e-01\n",
      "Task 9/10: loss.mean=7.929e-02, acc.mean=9.799e-01\n",
      "Task 10/10: loss.mean=3.948e-02, acc.mean=9.897e-01\n",
      "Validation: loss.mean=1.075e+00, acc.mean=7.100e-01\n",
      "\n",
      "-----Episode 11/100-----\n",
      "Task 1/10: loss.mean=2.915e-02, acc.mean=9.916e-01\n",
      "Task 2/10: loss.mean=3.120e-02, acc.mean=9.920e-01\n",
      "Task 3/10: loss.mean=8.765e-02, acc.mean=9.734e-01\n",
      "Task 4/10: loss.mean=6.888e-02, acc.mean=9.793e-01\n",
      "Task 5/10: loss.mean=6.698e-02, acc.mean=9.808e-01\n",
      "Task 6/10: loss.mean=1.015e-01, acc.mean=9.667e-01\n",
      "Task 7/10: loss.mean=1.246e-01, acc.mean=9.621e-01\n",
      "Task 8/10: loss.mean=9.299e-02, acc.mean=9.704e-01\n",
      "Task 9/10: loss.mean=1.205e-01, acc.mean=9.630e-01\n",
      "Task 10/10: loss.mean=6.391e-02, acc.mean=9.811e-01\n",
      "Validation: loss.mean=6.457e-01, acc.mean=8.060e-01\n",
      "\n",
      "-----Episode 12/100-----\n",
      "Task 1/10: loss.mean=8.917e-02, acc.mean=9.731e-01\n",
      "Task 2/10: loss.mean=3.294e-02, acc.mean=9.912e-01\n",
      "Task 3/10: loss.mean=5.509e-02, acc.mean=9.837e-01\n",
      "Task 4/10: loss.mean=1.671e-01, acc.mean=9.446e-01\n",
      "Task 5/10: loss.mean=1.074e-01, acc.mean=9.664e-01\n",
      "Task 6/10: loss.mean=6.306e-02, acc.mean=9.811e-01\n",
      "Task 7/10: loss.mean=5.959e-02, acc.mean=9.824e-01\n",
      "Task 8/10: loss.mean=6.755e-02, acc.mean=9.800e-01\n",
      "Task 9/10: loss.mean=6.436e-02, acc.mean=9.813e-01\n",
      "Task 10/10: loss.mean=2.864e-02, acc.mean=9.927e-01\n",
      "Validation: loss.mean=9.164e-01, acc.mean=7.360e-01\n",
      "\n",
      "-----Episode 13/100-----\n",
      "Task 1/10: loss.mean=1.012e-01, acc.mean=9.670e-01\n",
      "Task 2/10: loss.mean=8.174e-02, acc.mean=9.750e-01\n",
      "Task 3/10: loss.mean=7.578e-02, acc.mean=9.746e-01\n",
      "Task 4/10: loss.mean=8.622e-02, acc.mean=9.706e-01\n",
      "Task 5/10: loss.mean=1.026e-01, acc.mean=9.639e-01\n",
      "Task 6/10: loss.mean=7.538e-02, acc.mean=9.800e-01\n",
      "Task 7/10: loss.mean=1.955e-01, acc.mean=9.390e-01\n",
      "Task 8/10: loss.mean=8.270e-02, acc.mean=9.751e-01\n",
      "Task 9/10: loss.mean=1.154e-01, acc.mean=9.622e-01\n",
      "Task 10/10: loss.mean=1.374e-01, acc.mean=9.573e-01\n",
      "Validation: loss.mean=9.657e-01, acc.mean=7.073e-01\n",
      "\n",
      "-----Episode 14/100-----\n",
      "Task 1/10: loss.mean=1.089e-01, acc.mean=9.621e-01\n",
      "Task 2/10: loss.mean=1.171e-01, acc.mean=9.624e-01\n",
      "Task 3/10: loss.mean=6.400e-02, acc.mean=9.805e-01\n",
      "Task 4/10: loss.mean=1.409e-01, acc.mean=9.484e-01\n",
      "Task 5/10: loss.mean=1.158e-01, acc.mean=9.607e-01\n",
      "Task 6/10: loss.mean=2.244e-01, acc.mean=9.178e-01\n",
      "Task 7/10: loss.mean=9.370e-02, acc.mean=9.694e-01\n",
      "Task 8/10: loss.mean=3.876e-01, acc.mean=8.685e-01\n",
      "Task 9/10: loss.mean=9.797e-02, acc.mean=9.681e-01\n",
      "Task 10/10: loss.mean=1.914e-01, acc.mean=9.392e-01\n",
      "Validation: loss.mean=9.353e-01, acc.mean=7.227e-01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-----Episode 15/100-----\n",
      "Task 1/10: loss.mean=1.376e-01, acc.mean=9.556e-01\n",
      "Task 2/10: loss.mean=2.301e-01, acc.mean=9.221e-01\n",
      "Task 3/10: loss.mean=3.397e-01, acc.mean=8.901e-01\n",
      "Task 4/10: loss.mean=5.522e-01, acc.mean=8.382e-01\n",
      "Task 5/10: loss.mean=8.697e-02, acc.mean=9.696e-01\n",
      "Task 6/10: loss.mean=2.237e-01, acc.mean=9.234e-01\n",
      "Task 7/10: loss.mean=1.592e-01, acc.mean=9.431e-01\n",
      "Task 8/10: loss.mean=1.510e-01, acc.mean=9.496e-01\n",
      "Task 9/10: loss.mean=2.056e-01, acc.mean=9.277e-01\n",
      "Task 10/10: loss.mean=1.130e-01, acc.mean=9.636e-01\n",
      "Validation: loss.mean=6.454e-01, acc.mean=7.973e-01\n",
      "\n",
      "-----Episode 16/100-----\n",
      "Task 1/10: loss.mean=1.444e-01, acc.mean=9.474e-01\n",
      "Task 2/10: loss.mean=2.064e-01, acc.mean=9.325e-01\n",
      "Task 3/10: loss.mean=1.526e-01, acc.mean=9.495e-01\n",
      "Task 4/10: loss.mean=1.885e-01, acc.mean=9.288e-01\n",
      "Task 5/10: loss.mean=2.561e-01, acc.mean=9.148e-01\n",
      "Task 6/10: loss.mean=1.898e-01, acc.mean=9.320e-01\n",
      "Task 7/10: loss.mean=2.087e-01, acc.mean=9.341e-01\n",
      "Task 8/10: loss.mean=2.881e-01, acc.mean=8.967e-01\n",
      "Task 9/10: loss.mean=2.927e-01, acc.mean=8.962e-01\n",
      "Task 10/10: loss.mean=1.892e-01, acc.mean=9.377e-01\n",
      "Validation: loss.mean=9.039e-01, acc.mean=7.387e-01\n",
      "\n",
      "-----Episode 17/100-----\n",
      "Task 1/10: loss.mean=3.841e-01, acc.mean=8.613e-01\n",
      "Task 2/10: loss.mean=2.838e-01, acc.mean=8.988e-01\n",
      "Task 3/10: loss.mean=1.461e-01, acc.mean=9.533e-01\n",
      "Task 4/10: loss.mean=4.237e-01, acc.mean=8.456e-01\n",
      "Task 5/10: loss.mean=2.570e-01, acc.mean=9.150e-01\n",
      "Task 6/10: loss.mean=1.598e-01, acc.mean=9.443e-01\n",
      "Task 7/10: loss.mean=2.926e-01, acc.mean=8.916e-01\n",
      "Task 8/10: loss.mean=1.705e-01, acc.mean=9.422e-01\n",
      "Task 9/10: loss.mean=1.124e-01, acc.mean=9.597e-01\n",
      "Task 10/10: loss.mean=2.341e-01, acc.mean=9.216e-01\n",
      "Validation: loss.mean=1.000e+00, acc.mean=7.233e-01\n",
      "\n",
      "-----Episode 18/100-----\n",
      "Task 1/10: loss.mean=1.076e-01, acc.mean=9.627e-01\n",
      "Task 2/10: loss.mean=2.700e-01, acc.mean=8.998e-01\n",
      "Task 3/10: loss.mean=1.692e-01, acc.mean=9.491e-01\n",
      "Task 4/10: loss.mean=4.167e-01, acc.mean=8.579e-01\n",
      "Task 5/10: loss.mean=3.073e-01, acc.mean=8.927e-01\n",
      "Task 6/10: loss.mean=3.449e-01, acc.mean=8.807e-01\n",
      "Task 7/10: loss.mean=9.582e-02, acc.mean=9.667e-01\n",
      "Task 8/10: loss.mean=2.131e-01, acc.mean=9.236e-01\n",
      "Task 9/10: loss.mean=2.630e-01, acc.mean=9.077e-01\n",
      "Task 10/10: loss.mean=2.876e-01, acc.mean=9.118e-01\n",
      "Validation: loss.mean=6.043e-01, acc.mean=8.020e-01\n",
      "\n",
      "-----Episode 19/100-----\n",
      "Task 1/10: loss.mean=2.055e-01, acc.mean=9.390e-01\n",
      "Task 2/10: loss.mean=3.855e-01, acc.mean=8.773e-01\n",
      "Task 3/10: loss.mean=4.276e-01, acc.mean=8.583e-01\n",
      "Task 4/10: loss.mean=2.942e-01, acc.mean=9.002e-01\n",
      "Task 5/10: loss.mean=3.737e-01, acc.mean=8.594e-01\n",
      "Task 6/10: loss.mean=2.884e-01, acc.mean=8.927e-01\n",
      "Task 7/10: loss.mean=2.911e-01, acc.mean=8.870e-01\n",
      "Task 8/10: loss.mean=1.124e-01, acc.mean=9.609e-01\n",
      "Task 9/10: loss.mean=3.427e-01, acc.mean=8.808e-01\n",
      "Task 10/10: loss.mean=2.090e-01, acc.mean=9.285e-01\n",
      "Validation: loss.mean=6.218e-01, acc.mean=8.127e-01\n",
      "\n",
      "-----Episode 20/100-----\n",
      "Task 1/10: loss.mean=3.584e-01, acc.mean=8.724e-01\n",
      "Task 2/10: loss.mean=5.595e-01, acc.mean=8.543e-01\n",
      "Task 3/10: loss.mean=3.052e-01, acc.mean=8.917e-01\n",
      "Task 4/10: loss.mean=2.807e-01, acc.mean=9.083e-01\n",
      "Task 5/10: loss.mean=3.539e-01, acc.mean=8.796e-01\n",
      "Task 6/10: loss.mean=1.387e-01, acc.mean=9.564e-01\n",
      "Task 7/10: loss.mean=6.005e-01, acc.mean=7.859e-01\n",
      "Task 8/10: loss.mean=3.556e-01, acc.mean=8.757e-01\n",
      "Task 9/10: loss.mean=1.890e-01, acc.mean=9.376e-01\n",
      "Task 10/10: loss.mean=2.239e-01, acc.mean=9.202e-01\n",
      "Validation: loss.mean=6.453e-01, acc.mean=8.000e-01\n",
      "\n",
      "-----Episode 21/100-----\n",
      "Task 1/10: loss.mean=3.663e-01, acc.mean=8.590e-01\n",
      "Task 2/10: loss.mean=1.685e-01, acc.mean=9.419e-01\n",
      "Task 3/10: loss.mean=2.506e-01, acc.mean=9.086e-01\n",
      "Task 4/10: loss.mean=2.351e-01, acc.mean=9.238e-01\n",
      "Task 5/10: loss.mean=3.975e-01, acc.mean=8.585e-01\n",
      "Task 6/10: loss.mean=2.033e-01, acc.mean=9.374e-01\n",
      "Task 7/10: loss.mean=2.317e-01, acc.mean=9.083e-01\n",
      "Task 8/10: loss.mean=2.352e-01, acc.mean=9.047e-01\n",
      "Task 9/10: loss.mean=3.349e-01, acc.mean=8.663e-01\n",
      "Task 10/10: loss.mean=3.052e-01, acc.mean=8.936e-01\n",
      "Validation: loss.mean=7.211e-01, acc.mean=7.807e-01\n",
      "\n",
      "-----Episode 22/100-----\n",
      "Task 1/10: loss.mean=4.584e-01, acc.mean=8.428e-01\n",
      "Task 2/10: loss.mean=4.329e-01, acc.mean=8.627e-01\n",
      "Task 3/10: loss.mean=5.017e-01, acc.mean=8.284e-01\n",
      "Task 4/10: loss.mean=2.844e-01, acc.mean=9.016e-01\n",
      "Task 5/10: loss.mean=1.926e-01, acc.mean=9.348e-01\n",
      "Task 6/10: loss.mean=3.323e-01, acc.mean=8.910e-01\n",
      "Task 7/10: loss.mean=3.909e-01, acc.mean=8.546e-01\n",
      "Task 8/10: loss.mean=2.891e-01, acc.mean=8.966e-01\n",
      "Task 9/10: loss.mean=3.840e-01, acc.mean=8.675e-01\n",
      "Task 10/10: loss.mean=4.571e-01, acc.mean=8.357e-01\n",
      "Validation: loss.mean=7.999e-01, acc.mean=7.593e-01\n",
      "\n",
      "-----Episode 23/100-----\n",
      "Task 1/10: loss.mean=2.712e-01, acc.mean=9.026e-01\n",
      "Task 2/10: loss.mean=4.385e-01, acc.mean=8.711e-01\n",
      "Task 3/10: loss.mean=2.345e-01, acc.mean=9.095e-01\n",
      "Task 4/10: loss.mean=1.763e-01, acc.mean=9.355e-01\n",
      "Task 5/10: loss.mean=3.813e-01, acc.mean=8.553e-01\n",
      "Task 6/10: loss.mean=3.053e-01, acc.mean=8.927e-01\n",
      "Task 7/10: loss.mean=2.985e-01, acc.mean=8.955e-01\n",
      "Task 8/10: loss.mean=6.110e-01, acc.mean=7.853e-01\n",
      "Task 9/10: loss.mean=2.767e-01, acc.mean=9.121e-01\n",
      "Task 10/10: loss.mean=4.712e-01, acc.mean=8.376e-01\n",
      "Validation: loss.mean=6.367e-01, acc.mean=7.953e-01\n",
      "\n",
      "-----Episode 24/100-----\n",
      "Task 1/10: loss.mean=4.049e-01, acc.mean=8.578e-01\n",
      "Task 2/10: loss.mean=6.114e-01, acc.mean=7.661e-01\n",
      "Task 3/10: loss.mean=4.025e-01, acc.mean=8.441e-01\n",
      "Task 4/10: loss.mean=1.895e-01, acc.mean=9.346e-01\n",
      "Task 5/10: loss.mean=3.378e-01, acc.mean=8.747e-01\n",
      "Task 6/10: loss.mean=2.970e-01, acc.mean=8.915e-01\n",
      "Task 7/10: loss.mean=2.668e-01, acc.mean=8.962e-01\n",
      "Task 8/10: loss.mean=4.335e-01, acc.mean=8.576e-01\n",
      "Task 9/10: loss.mean=1.855e-01, acc.mean=9.296e-01\n",
      "Task 10/10: loss.mean=5.210e-01, acc.mean=8.501e-01\n",
      "Validation: loss.mean=9.056e-01, acc.mean=7.387e-01\n",
      "\n",
      "-----Episode 25/100-----\n",
      "Task 1/10: loss.mean=2.126e-01, acc.mean=9.243e-01\n",
      "Task 2/10: loss.mean=4.268e-01, acc.mean=8.629e-01\n",
      "Task 3/10: loss.mean=5.406e-01, acc.mean=8.199e-01\n",
      "Task 4/10: loss.mean=7.098e-01, acc.mean=7.571e-01\n",
      "Task 5/10: loss.mean=4.043e-01, acc.mean=8.498e-01\n",
      "Task 6/10: loss.mean=4.125e-01, acc.mean=8.639e-01\n",
      "Task 7/10: loss.mean=2.592e-01, acc.mean=9.260e-01\n",
      "Task 8/10: loss.mean=3.906e-01, acc.mean=8.768e-01\n",
      "Task 9/10: loss.mean=3.888e-01, acc.mean=8.645e-01\n",
      "Task 10/10: loss.mean=3.520e-01, acc.mean=8.639e-01\n",
      "Validation: loss.mean=6.473e-01, acc.mean=8.080e-01\n",
      "\n",
      "-----Episode 26/100-----\n",
      "Task 1/10: loss.mean=2.263e-01, acc.mean=9.263e-01\n",
      "Task 2/10: loss.mean=1.711e-01, acc.mean=9.376e-01\n",
      "Task 3/10: loss.mean=5.332e-01, acc.mean=8.117e-01\n",
      "Task 4/10: loss.mean=5.776e-01, acc.mean=7.799e-01\n",
      "Task 5/10: loss.mean=4.372e-01, acc.mean=8.494e-01\n",
      "Task 6/10: loss.mean=2.643e-01, acc.mean=9.150e-01\n",
      "Task 7/10: loss.mean=4.029e-01, acc.mean=8.944e-01\n",
      "Task 8/10: loss.mean=3.063e-01, acc.mean=8.899e-01\n",
      "Task 9/10: loss.mean=5.016e-01, acc.mean=8.135e-01\n",
      "Task 10/10: loss.mean=4.314e-01, acc.mean=8.590e-01\n",
      "Validation: loss.mean=6.187e-01, acc.mean=8.007e-01\n",
      "\n",
      "-----Episode 27/100-----\n",
      "Task 1/10: loss.mean=5.915e-01, acc.mean=8.467e-01\n",
      "Task 2/10: loss.mean=4.575e-01, acc.mean=8.599e-01\n",
      "Task 3/10: loss.mean=3.264e-01, acc.mean=8.799e-01\n",
      "Task 4/10: loss.mean=4.426e-01, acc.mean=8.639e-01\n",
      "Task 5/10: loss.mean=2.036e-01, acc.mean=9.245e-01\n",
      "Task 6/10: loss.mean=2.092e-01, acc.mean=9.213e-01\n",
      "Task 7/10: loss.mean=4.371e-01, acc.mean=8.563e-01\n",
      "Task 8/10: loss.mean=4.218e-01, acc.mean=8.640e-01\n",
      "Task 9/10: loss.mean=2.649e-01, acc.mean=9.140e-01\n",
      "Task 10/10: loss.mean=2.743e-01, acc.mean=8.993e-01\n",
      "Validation: loss.mean=7.275e-01, acc.mean=7.733e-01\n",
      "\n",
      "-----Episode 28/100-----\n",
      "Task 1/10: loss.mean=2.333e-01, acc.mean=9.211e-01\n",
      "Task 2/10: loss.mean=3.772e-01, acc.mean=8.714e-01\n",
      "Task 3/10: loss.mean=2.401e-01, acc.mean=9.301e-01\n",
      "Task 4/10: loss.mean=2.971e-01, acc.mean=8.902e-01\n",
      "Task 5/10: loss.mean=3.913e-01, acc.mean=8.806e-01\n",
      "Task 6/10: loss.mean=4.101e-01, acc.mean=8.624e-01\n",
      "Task 7/10: loss.mean=5.028e-01, acc.mean=8.399e-01\n",
      "Task 8/10: loss.mean=4.469e-01, acc.mean=8.561e-01\n",
      "Task 9/10: loss.mean=3.501e-01, acc.mean=8.768e-01\n",
      "Task 10/10: loss.mean=8.471e-01, acc.mean=7.346e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: loss.mean=7.132e-01, acc.mean=7.733e-01\n",
      "\n",
      "-----Episode 29/100-----\n",
      "Task 1/10: loss.mean=3.588e-01, acc.mean=8.846e-01\n",
      "Task 2/10: loss.mean=2.197e-01, acc.mean=9.138e-01\n",
      "Task 3/10: loss.mean=4.137e-01, acc.mean=8.545e-01\n",
      "Task 4/10: loss.mean=4.080e-01, acc.mean=8.715e-01\n",
      "Task 5/10: loss.mean=3.162e-01, acc.mean=8.808e-01\n",
      "Task 6/10: loss.mean=3.746e-01, acc.mean=8.500e-01\n",
      "Task 7/10: loss.mean=2.655e-01, acc.mean=9.059e-01\n",
      "Task 8/10: loss.mean=1.882e-01, acc.mean=9.308e-01\n",
      "Task 9/10: loss.mean=5.763e-01, acc.mean=7.883e-01\n",
      "Task 10/10: loss.mean=2.056e-01, acc.mean=9.367e-01\n",
      "Validation: loss.mean=7.269e-01, acc.mean=7.600e-01\n",
      "\n",
      "-----Episode 30/100-----\n",
      "Task 1/10: loss.mean=5.565e-01, acc.mean=8.197e-01\n",
      "Task 2/10: loss.mean=3.260e-01, acc.mean=8.762e-01\n",
      "Task 3/10: loss.mean=8.898e-01, acc.mean=7.216e-01\n",
      "Task 4/10: loss.mean=2.957e-01, acc.mean=8.982e-01\n",
      "Task 5/10: loss.mean=4.104e-01, acc.mean=8.663e-01\n",
      "Task 6/10: loss.mean=6.305e-01, acc.mean=8.054e-01\n",
      "Task 7/10: loss.mean=1.640e-01, acc.mean=9.395e-01\n",
      "Task 8/10: loss.mean=4.501e-01, acc.mean=8.694e-01\n",
      "Task 9/10: loss.mean=4.809e-01, acc.mean=8.404e-01\n",
      "Task 10/10: loss.mean=1.194e-01, acc.mean=9.610e-01\n",
      "Validation: loss.mean=6.898e-01, acc.mean=7.813e-01\n",
      "\n",
      "-----Episode 31/100-----\n",
      "Task 1/10: loss.mean=3.225e-01, acc.mean=8.818e-01\n",
      "Task 2/10: loss.mean=3.400e-01, acc.mean=8.835e-01\n",
      "Task 3/10: loss.mean=2.744e-01, acc.mean=9.103e-01\n",
      "Task 4/10: loss.mean=4.956e-01, acc.mean=8.313e-01\n",
      "Task 5/10: loss.mean=1.269e-01, acc.mean=9.504e-01\n",
      "Task 6/10: loss.mean=1.716e-01, acc.mean=9.418e-01\n",
      "Task 7/10: loss.mean=2.289e-01, acc.mean=9.234e-01\n",
      "Task 8/10: loss.mean=1.527e-01, acc.mean=9.440e-01\n",
      "Task 9/10: loss.mean=6.186e-01, acc.mean=8.044e-01\n",
      "Task 10/10: loss.mean=5.537e-01, acc.mean=8.106e-01\n",
      "Validation: loss.mean=5.434e-01, acc.mean=8.127e-01\n",
      "\n",
      "-----Episode 32/100-----\n",
      "Task 1/10: loss.mean=5.346e-01, acc.mean=8.356e-01\n",
      "Task 2/10: loss.mean=5.570e-01, acc.mean=8.359e-01\n",
      "Task 3/10: loss.mean=3.116e-01, acc.mean=8.900e-01\n",
      "Task 4/10: loss.mean=1.751e-01, acc.mean=9.381e-01\n",
      "Task 5/10: loss.mean=4.796e-01, acc.mean=8.195e-01\n",
      "Task 6/10: loss.mean=2.999e-01, acc.mean=8.965e-01\n",
      "Task 7/10: loss.mean=1.246e-01, acc.mean=9.565e-01\n",
      "Task 8/10: loss.mean=4.283e-01, acc.mean=8.659e-01\n",
      "Task 9/10: loss.mean=1.410e-01, acc.mean=9.546e-01\n",
      "Task 10/10: loss.mean=3.931e-01, acc.mean=8.444e-01\n",
      "Validation: loss.mean=6.991e-01, acc.mean=7.833e-01\n",
      "\n",
      "-----Episode 33/100-----\n",
      "Task 1/10: loss.mean=6.192e-01, acc.mean=7.801e-01\n",
      "Task 2/10: loss.mean=2.176e-01, acc.mean=9.247e-01\n",
      "Task 3/10: loss.mean=3.378e-01, acc.mean=8.719e-01\n",
      "Task 4/10: loss.mean=2.941e-01, acc.mean=8.905e-01\n",
      "Task 5/10: loss.mean=3.013e-01, acc.mean=9.011e-01\n",
      "Task 6/10: loss.mean=4.763e-01, acc.mean=8.191e-01\n",
      "Task 7/10: loss.mean=2.169e-01, acc.mean=9.170e-01\n",
      "Task 8/10: loss.mean=5.236e-01, acc.mean=8.099e-01\n",
      "Task 9/10: loss.mean=1.237e-01, acc.mean=9.615e-01\n",
      "Task 10/10: loss.mean=1.389e-01, acc.mean=9.558e-01\n",
      "Validation: loss.mean=6.528e-01, acc.mean=7.973e-01\n",
      "\n",
      "-----Episode 34/100-----\n",
      "Task 1/10: loss.mean=4.586e-01, acc.mean=8.524e-01\n",
      "Task 2/10: loss.mean=3.915e-01, acc.mean=8.780e-01\n",
      "Task 3/10: loss.mean=1.944e-01, acc.mean=9.322e-01\n",
      "Task 4/10: loss.mean=6.197e-01, acc.mean=8.055e-01\n",
      "Task 5/10: loss.mean=3.832e-01, acc.mean=8.456e-01\n",
      "Task 6/10: loss.mean=2.117e-01, acc.mean=9.201e-01\n",
      "Task 7/10: loss.mean=5.576e-01, acc.mean=8.327e-01\n",
      "Task 8/10: loss.mean=4.345e-01, acc.mean=8.243e-01\n",
      "Task 9/10: loss.mean=5.132e-01, acc.mean=8.213e-01\n",
      "Task 10/10: loss.mean=4.890e-01, acc.mean=8.312e-01\n",
      "Validation: loss.mean=6.945e-01, acc.mean=7.813e-01\n",
      "\n",
      "-----Episode 35/100-----\n",
      "Task 1/10: loss.mean=3.224e-01, acc.mean=8.846e-01\n",
      "Task 2/10: loss.mean=5.001e-01, acc.mean=8.258e-01\n",
      "Task 3/10: loss.mean=4.491e-01, acc.mean=8.416e-01\n",
      "Task 4/10: loss.mean=1.622e-01, acc.mean=9.381e-01\n",
      "Task 5/10: loss.mean=4.910e-01, acc.mean=8.196e-01\n",
      "Task 6/10: loss.mean=5.091e-01, acc.mean=8.320e-01\n",
      "Task 7/10: loss.mean=5.720e-01, acc.mean=7.844e-01\n",
      "Task 8/10: loss.mean=3.064e-01, acc.mean=8.891e-01\n",
      "Task 9/10: loss.mean=2.011e-01, acc.mean=9.311e-01\n",
      "Task 10/10: loss.mean=6.382e-01, acc.mean=7.771e-01\n",
      "Validation: loss.mean=6.881e-01, acc.mean=7.713e-01\n",
      "\n",
      "-----Episode 36/100-----\n",
      "Task 1/10: loss.mean=4.765e-01, acc.mean=8.416e-01\n",
      "Task 2/10: loss.mean=4.049e-01, acc.mean=8.475e-01\n",
      "Task 3/10: loss.mean=3.079e-01, acc.mean=9.013e-01\n",
      "Task 4/10: loss.mean=4.523e-01, acc.mean=8.706e-01\n",
      "Task 5/10: loss.mean=4.016e-01, acc.mean=8.459e-01\n",
      "Task 6/10: loss.mean=6.032e-01, acc.mean=8.027e-01\n",
      "Task 7/10: loss.mean=6.010e-01, acc.mean=8.175e-01\n",
      "Task 8/10: loss.mean=4.547e-01, acc.mean=8.389e-01\n",
      "Task 9/10: loss.mean=4.884e-01, acc.mean=8.240e-01\n",
      "Task 10/10: loss.mean=6.217e-01, acc.mean=7.811e-01\n",
      "Validation: loss.mean=6.856e-01, acc.mean=7.693e-01\n",
      "\n",
      "-----Episode 37/100-----\n",
      "Task 1/10: loss.mean=4.733e-01, acc.mean=8.397e-01\n",
      "Task 2/10: loss.mean=4.808e-01, acc.mean=8.349e-01\n",
      "Task 3/10: loss.mean=5.528e-01, acc.mean=8.043e-01\n",
      "Task 4/10: loss.mean=4.955e-01, acc.mean=8.339e-01\n",
      "Task 5/10: loss.mean=8.904e-01, acc.mean=7.704e-01\n",
      "Task 6/10: loss.mean=5.663e-01, acc.mean=8.151e-01\n",
      "Task 7/10: loss.mean=4.115e-01, acc.mean=8.544e-01\n",
      "Task 8/10: loss.mean=4.873e-01, acc.mean=8.345e-01\n",
      "Task 9/10: loss.mean=2.444e-01, acc.mean=9.102e-01\n",
      "Task 10/10: loss.mean=5.020e-01, acc.mean=8.060e-01\n",
      "Validation: loss.mean=7.115e-01, acc.mean=7.727e-01\n",
      "\n",
      "-----Episode 38/100-----\n",
      "Task 1/10: loss.mean=3.555e-01, acc.mean=8.923e-01\n",
      "Task 2/10: loss.mean=5.196e-01, acc.mean=8.187e-01\n",
      "Task 3/10: loss.mean=6.654e-01, acc.mean=8.067e-01\n",
      "Task 4/10: loss.mean=4.489e-01, acc.mean=8.327e-01\n",
      "Task 5/10: loss.mean=5.784e-01, acc.mean=8.176e-01\n",
      "Task 6/10: loss.mean=5.576e-01, acc.mean=8.382e-01\n",
      "Task 7/10: loss.mean=6.465e-01, acc.mean=7.913e-01\n",
      "Task 8/10: loss.mean=4.214e-01, acc.mean=8.405e-01\n",
      "Task 9/10: loss.mean=4.218e-01, acc.mean=8.491e-01\n",
      "Task 10/10: loss.mean=4.258e-01, acc.mean=8.548e-01\n",
      "Validation: loss.mean=5.469e-01, acc.mean=8.167e-01\n",
      "\n",
      "-----Episode 39/100-----\n",
      "Task 1/10: loss.mean=2.144e-01, acc.mean=9.257e-01\n",
      "Task 2/10: loss.mean=2.093e-01, acc.mean=9.236e-01\n",
      "Task 3/10: loss.mean=3.012e-01, acc.mean=9.077e-01\n",
      "Task 4/10: loss.mean=3.351e-01, acc.mean=9.087e-01\n",
      "Task 5/10: loss.mean=3.962e-01, acc.mean=8.778e-01\n",
      "Task 6/10: loss.mean=3.711e-01, acc.mean=8.649e-01\n",
      "Task 7/10: loss.mean=4.671e-01, acc.mean=8.388e-01\n",
      "Task 8/10: loss.mean=7.220e-01, acc.mean=7.646e-01\n",
      "Task 9/10: loss.mean=4.658e-01, acc.mean=8.420e-01\n",
      "Task 10/10: loss.mean=3.275e-01, acc.mean=8.991e-01\n",
      "Validation: loss.mean=5.354e-01, acc.mean=8.327e-01\n",
      "\n",
      "-----Episode 40/100-----\n",
      "Task 1/10: loss.mean=3.764e-01, acc.mean=8.658e-01\n",
      "Task 2/10: loss.mean=4.651e-01, acc.mean=8.373e-01\n",
      "Task 3/10: loss.mean=4.333e-01, acc.mean=8.426e-01\n",
      "Task 4/10: loss.mean=3.635e-01, acc.mean=8.634e-01\n",
      "Task 5/10: loss.mean=6.637e-01, acc.mean=7.753e-01\n",
      "Task 6/10: loss.mean=4.453e-01, acc.mean=8.407e-01\n",
      "Task 7/10: loss.mean=2.864e-01, acc.mean=8.986e-01\n",
      "Task 8/10: loss.mean=9.255e-01, acc.mean=7.147e-01\n",
      "Task 9/10: loss.mean=4.138e-01, acc.mean=8.422e-01\n",
      "Task 10/10: loss.mean=4.832e-01, acc.mean=8.438e-01\n",
      "Validation: loss.mean=5.845e-01, acc.mean=8.033e-01\n",
      "\n",
      "-----Episode 41/100-----\n",
      "Task 1/10: loss.mean=4.254e-01, acc.mean=8.695e-01\n",
      "Task 2/10: loss.mean=2.507e-01, acc.mean=9.128e-01\n",
      "Task 3/10: loss.mean=4.568e-01, acc.mean=8.439e-01\n",
      "Task 4/10: loss.mean=3.499e-01, acc.mean=8.684e-01\n",
      "Task 5/10: loss.mean=2.031e-01, acc.mean=9.276e-01\n",
      "Task 6/10: loss.mean=1.546e-01, acc.mean=9.421e-01\n",
      "Task 7/10: loss.mean=8.253e-01, acc.mean=7.622e-01\n",
      "Task 8/10: loss.mean=5.043e-01, acc.mean=8.452e-01\n",
      "Task 9/10: loss.mean=1.690e-01, acc.mean=9.407e-01\n",
      "Task 10/10: loss.mean=4.547e-01, acc.mean=8.653e-01\n",
      "Validation: loss.mean=6.316e-01, acc.mean=7.993e-01\n",
      "\n",
      "-----Episode 42/100-----\n",
      "Task 1/10: loss.mean=3.844e-01, acc.mean=8.841e-01\n",
      "Task 2/10: loss.mean=5.307e-01, acc.mean=8.067e-01\n",
      "Task 3/10: loss.mean=2.307e-01, acc.mean=9.200e-01\n",
      "Task 4/10: loss.mean=3.917e-01, acc.mean=8.579e-01\n",
      "Task 5/10: loss.mean=5.762e-01, acc.mean=7.948e-01\n",
      "Task 6/10: loss.mean=3.842e-01, acc.mean=8.719e-01\n",
      "Task 7/10: loss.mean=3.354e-01, acc.mean=8.888e-01\n",
      "Task 8/10: loss.mean=6.098e-01, acc.mean=8.000e-01\n",
      "Task 9/10: loss.mean=7.577e-01, acc.mean=7.402e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 10/10: loss.mean=3.297e-01, acc.mean=9.005e-01\n",
      "Validation: loss.mean=6.431e-01, acc.mean=7.907e-01\n",
      "\n",
      "-----Episode 43/100-----\n",
      "Task 1/10: loss.mean=3.323e-01, acc.mean=8.830e-01\n",
      "Task 2/10: loss.mean=1.328e-01, acc.mean=9.491e-01\n",
      "Task 3/10: loss.mean=1.929e-01, acc.mean=9.326e-01\n",
      "Task 4/10: loss.mean=4.046e-01, acc.mean=8.617e-01\n",
      "Task 5/10: loss.mean=4.825e-01, acc.mean=8.365e-01\n",
      "Task 6/10: loss.mean=2.849e-01, acc.mean=8.971e-01\n",
      "Task 7/10: loss.mean=4.576e-01, acc.mean=8.547e-01\n",
      "Task 8/10: loss.mean=2.070e-01, acc.mean=9.389e-01\n",
      "Task 9/10: loss.mean=2.881e-01, acc.mean=8.952e-01\n",
      "Task 10/10: loss.mean=1.709e-01, acc.mean=9.389e-01\n",
      "Validation: loss.mean=5.825e-01, acc.mean=8.093e-01\n",
      "\n",
      "-----Episode 44/100-----\n",
      "Task 1/10: loss.mean=3.924e-01, acc.mean=8.462e-01\n",
      "Task 2/10: loss.mean=5.241e-01, acc.mean=8.105e-01\n",
      "Task 3/10: loss.mean=1.950e-01, acc.mean=9.261e-01\n",
      "Task 4/10: loss.mean=2.053e-01, acc.mean=9.276e-01\n",
      "Task 5/10: loss.mean=5.844e-01, acc.mean=8.060e-01\n",
      "Task 6/10: loss.mean=1.966e-01, acc.mean=9.363e-01\n",
      "Task 7/10: loss.mean=4.917e-01, acc.mean=8.436e-01\n",
      "Task 8/10: loss.mean=4.432e-01, acc.mean=8.492e-01\n",
      "Task 9/10: loss.mean=2.663e-01, acc.mean=9.174e-01\n",
      "Task 10/10: loss.mean=2.296e-01, acc.mean=9.086e-01\n",
      "Validation: loss.mean=6.991e-01, acc.mean=7.873e-01\n",
      "\n",
      "-----Episode 45/100-----\n",
      "Task 1/10: loss.mean=4.299e-01, acc.mean=8.391e-01\n",
      "Task 2/10: loss.mean=4.195e-01, acc.mean=8.384e-01\n",
      "Task 3/10: loss.mean=1.918e-01, acc.mean=9.252e-01\n",
      "Task 4/10: loss.mean=5.335e-01, acc.mean=7.918e-01\n",
      "Task 5/10: loss.mean=4.410e-01, acc.mean=8.715e-01\n",
      "Task 6/10: loss.mean=4.604e-01, acc.mean=8.404e-01\n",
      "Task 7/10: loss.mean=2.657e-01, acc.mean=9.111e-01\n",
      "Task 8/10: loss.mean=3.540e-01, acc.mean=8.699e-01\n",
      "Task 9/10: loss.mean=3.903e-01, acc.mean=8.657e-01\n",
      "Task 10/10: loss.mean=6.524e-01, acc.mean=8.208e-01\n",
      "Validation: loss.mean=6.676e-01, acc.mean=7.807e-01\n",
      "\n",
      "-----Episode 46/100-----\n",
      "Task 1/10: loss.mean=2.790e-01, acc.mean=9.026e-01\n",
      "Task 2/10: loss.mean=3.773e-01, acc.mean=8.654e-01\n",
      "Task 3/10: loss.mean=2.152e-01, acc.mean=9.205e-01\n",
      "Task 4/10: loss.mean=4.653e-01, acc.mean=8.328e-01\n",
      "Task 5/10: loss.mean=3.722e-01, acc.mean=8.581e-01\n",
      "Task 6/10: loss.mean=2.029e-01, acc.mean=9.238e-01\n",
      "Task 7/10: loss.mean=4.852e-01, acc.mean=8.661e-01\n",
      "Task 8/10: loss.mean=2.851e-01, acc.mean=8.816e-01\n",
      "Task 9/10: loss.mean=4.350e-01, acc.mean=8.408e-01\n",
      "Task 10/10: loss.mean=4.432e-01, acc.mean=8.476e-01\n",
      "Validation: loss.mean=7.353e-01, acc.mean=7.567e-01\n",
      "\n",
      "-----Episode 47/100-----\n",
      "Task 1/10: loss.mean=5.374e-01, acc.mean=8.234e-01\n",
      "Task 2/10: loss.mean=3.299e-01, acc.mean=8.818e-01\n",
      "Task 3/10: loss.mean=2.943e-01, acc.mean=8.987e-01\n",
      "Task 4/10: loss.mean=2.043e-01, acc.mean=9.249e-01\n",
      "Task 5/10: loss.mean=1.825e-01, acc.mean=9.322e-01\n",
      "Task 6/10: loss.mean=4.327e-01, acc.mean=8.386e-01\n",
      "Task 7/10: loss.mean=1.791e-01, acc.mean=9.357e-01\n",
      "Task 8/10: loss.mean=3.371e-01, acc.mean=8.937e-01\n",
      "Task 9/10: loss.mean=3.649e-01, acc.mean=8.807e-01\n",
      "Task 10/10: loss.mean=2.120e-01, acc.mean=9.219e-01\n",
      "Validation: loss.mean=6.011e-01, acc.mean=8.047e-01\n",
      "\n",
      "-----Episode 48/100-----\n",
      "Task 1/10: loss.mean=3.101e-01, acc.mean=8.859e-01\n",
      "Task 2/10: loss.mean=3.251e-01, acc.mean=8.761e-01\n",
      "Task 3/10: loss.mean=2.077e-01, acc.mean=9.118e-01\n",
      "Task 4/10: loss.mean=2.716e-01, acc.mean=9.054e-01\n",
      "Task 5/10: loss.mean=4.305e-01, acc.mean=8.548e-01\n",
      "Task 6/10: loss.mean=3.196e-01, acc.mean=8.825e-01\n",
      "Task 7/10: loss.mean=2.448e-01, acc.mean=9.126e-01\n",
      "Task 8/10: loss.mean=3.864e-01, acc.mean=8.709e-01\n",
      "Task 9/10: loss.mean=3.084e-01, acc.mean=8.887e-01\n",
      "Task 10/10: loss.mean=1.907e-01, acc.mean=9.308e-01\n",
      "Validation: loss.mean=7.483e-01, acc.mean=7.700e-01\n",
      "\n",
      "-----Episode 49/100-----\n",
      "Task 1/10: loss.mean=3.661e-01, acc.mean=8.661e-01\n",
      "Task 2/10: loss.mean=2.933e-01, acc.mean=9.021e-01\n",
      "Task 3/10: loss.mean=3.794e-01, acc.mean=8.836e-01\n",
      "Task 4/10: loss.mean=5.588e-01, acc.mean=8.212e-01\n",
      "Task 5/10: loss.mean=2.372e-01, acc.mean=9.189e-01\n",
      "Task 6/10: loss.mean=3.126e-01, acc.mean=8.985e-01\n",
      "Task 7/10: loss.mean=5.409e-01, acc.mean=8.126e-01\n",
      "Task 8/10: loss.mean=6.973e-01, acc.mean=7.752e-01\n",
      "Task 9/10: loss.mean=3.579e-01, acc.mean=8.689e-01\n",
      "Task 10/10: loss.mean=2.799e-01, acc.mean=8.961e-01\n",
      "Validation: loss.mean=6.492e-01, acc.mean=7.793e-01\n",
      "\n",
      "-----Episode 50/100-----\n",
      "Task 1/10: loss.mean=2.110e-01, acc.mean=9.189e-01\n",
      "Task 2/10: loss.mean=2.393e-01, acc.mean=9.253e-01\n",
      "Task 3/10: loss.mean=4.987e-01, acc.mean=8.321e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_172955/2950253441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# compute prototype of features for support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;31m# compute features for query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_172955/2950253441.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# compute prototype of features for support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;31m# compute features for query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Few Shot Learning/Codes/few-shot-learning-benchmark/fsl/models/torch/prototypical_networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev38_pip/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxiter_episode = 100\n",
    "maxiter_task = 10\n",
    "maxiter_test = 10\n",
    "maxiter_sample = 1000\n",
    "\n",
    "for i in range(maxiter_episode):  # loop over the episode\n",
    "    Logger.info(f'-----Episode {i+1}/{maxiter_episode}-----')\n",
    "    # train loop\n",
    "    model.train()\n",
    "        \n",
    "    # a task in meta-learning is equivalent to a mini-batch in normal learning\n",
    "    for t in range(maxiter_task):        \n",
    "        # draw a task, use sample_task_local() to generate classes at the local level.\n",
    "        task = train_sampler.sample_task()        \n",
    "        \n",
    "        Loss, Acc = [], []\n",
    "#         loss_func = None\n",
    "                \n",
    "        for n in range(maxiter_sample):  # maxiter_samples <-> mini-batch size            \n",
    "            # draw support-query samples\n",
    "            (sa, sl), (qa, ql) = train_sampler.sample_support_query(task, split=True, offclass=False, \\\n",
    "                                                                    **kwargs_imread)                        \n",
    "            # assert sl==ql, labels will not be used\n",
    "            # assert len(sa)==len(sl) and len(qa)==len(ql)                            \n",
    "        \n",
    "            # compute prototype of features for support\n",
    "            S = torch.stack([model.forward(x).mean(axis=0) for x in sa])\n",
    "            # compute features for query \n",
    "            Q = model.forward(qa.reshape((-1, *qa.shape[2:])))\n",
    "            loss, acc = model.loss_acc(Q, S)\n",
    "#             try:\n",
    "#                 loss_func += loss\n",
    "#             except:\n",
    "#                 loss_func = loss\n",
    "            \n",
    "#             if n%10 == 0:\n",
    "#                 Logger.info(f'Sample {n}/{maxiter_sample}: loss={loss.item():.3e}, acc={acc.item():.3e}')\n",
    "            \n",
    "            Loss.append(loss.item())\n",
    "            Acc.append(acc.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "        Logger.info(f'Task {t+1}/{maxiter_task}: loss.mean={np.mean(Loss):.3e}, acc.mean={np.mean(Acc):.3e}')\n",
    "        \n",
    "#         # Gradient step outside the sample loop is infeasible due to the memory overflow of loss_func\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_func.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "    # validation loop\n",
    "#     model.eval()  # same as net.train(False), gives wrong results    \n",
    "    with torch.no_grad():\n",
    "        Loss, Acc = [], []\n",
    "        for t in range(maxiter_test):\n",
    "            # draw a task\n",
    "            task = test_sampler.sample_task()        \n",
    "\n",
    "            # draw support-query samples\n",
    "            (sa, sl), (qa, ql) = test_sampler.sample_support_query(task, split=True, offclass=False, \\\n",
    "                                                                   **kwargs_imread)                        \n",
    "            # compute prototype of features for support\n",
    "            S = torch.stack([model.forward(x).mean(axis=0) for x in sa])\n",
    "            # compute features for query \n",
    "            Q = model.forward(qa.reshape((-1, *qa.shape[2:])))\n",
    "            loss, acc = model.loss_acc(Q, S)\n",
    "\n",
    "            Loss.append(loss.item())\n",
    "            Acc.append(acc.item())\n",
    "\n",
    "        Logger.info(f'Validation: loss.mean={np.mean(Loss):.3e}, acc.mean={np.mean(Acc):.3e}\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767e976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "torch.save(model, os.path.join(outdir, 'model_torch.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d82e34",
   "metadata": {},
   "source": [
    "## EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058d36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev38_pip)",
   "language": "python",
   "name": "dev38_pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
